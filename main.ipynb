{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Let's get started!\n",
    "\n",
    "### Design decisions?\n",
    "- Punctuation looks like it's important, so I think we should keep it? like `!!!!` could actually be indicative of something, as might `???`. Periods, maybe not so much, so I think I'm gonna remove those, commas too\n",
    "- It might be better to split up contractions, so I thought that could be n i c e\n",
    "- iffy on stemming and lemmatization. Vader doesn't like it, but we can try it?\n",
    "    - lemmatization: spacy\n",
    "- or, try two versions, one with and the other without, see which one performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our imports\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants should we want that\n",
    "\n",
    "# taken from wikipedia + stackexchange answer: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall / I will\",\n",
    "    \"I'll've\": \"I shall have / I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of set up\n",
    "lemmatization_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of parsedEntries\n",
    "def makeListEntries(filename):\n",
    "    data = [json.loads(line) for line in open(filename, 'r')]\n",
    "    \n",
    "    for entry in data:\n",
    "        entry['review_body'] = entry['review_body'].lower()\n",
    "        \n",
    "        # taking out contractions\n",
    "        for key in contractions:\n",
    "            entry['review_body'] = re.sub(key, contractions[key], entry['review_body'])\n",
    "        \n",
    "        entry['tokenized'] = []\n",
    "        \n",
    "        tokens = lemmatization_model(entry['review_body'])\n",
    "        \n",
    "        # removing unnecessary punctuation\n",
    "        tokens = lemmatization_model(entry['review_body'])\n",
    "        entry['tokenized'] = [token.lemma_ for token in tokens if token.lemma_ not in {',', '.'}]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = makeListEntries(\"dataset/smol_train.json\")\n",
    "testSet = makeListEntries(\"dataset/smol_test.json\")\n",
    "\n",
    "#trainSet = makeListEntries(\"dataset/dataset_en_train.json\")\n",
    "#testSet = makeListEntries(\"dataset/dataset_en_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    pass\n",
    "    #print(trainSet[i]['tokenized'])\n",
    "    #print(testSet[i]['tokenized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the list of words a string, adds that to a list\n",
    "def makeListText(dataSet):\n",
    "    resList = []\n",
    "    for entry in dataSet:\n",
    "        resList.append(\" \".join(entry['tokenized']))\n",
    "    return resList\n",
    "\n",
    "# deal with target (the stars) as well\n",
    "def makeListStars(dataSet):\n",
    "    resList = []\n",
    "    for entry in dataSet:\n",
    "        resList.append(entry['stars'])\n",
    "    return resList\n",
    "\n",
    "# data\n",
    "listTrainText = makeListText(trainSet)\n",
    "listTestText = makeListText(testSet)\n",
    "\n",
    "# target\n",
    "listTrainStars = makeListStars(trainSet)\n",
    "listTestStars = makeListStars(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could do CountVectorizer\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "trainCVMatr = cv.fit_transform(listTrainText)\n",
    "testCVMatr = cv.transform(listTestText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TdidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could do TfidfVectorizer\n",
    "tv = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "trainTVMatr = cv.fit_transform(listTrainText)\n",
    "testTVMatr = cv.transform(listTestText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass, logistic regression, CountVectorizer: 0.48\n",
      "Multiclass, logistic regression, TfidfVectorizer: 0.48\n"
     ]
    }
   ],
   "source": [
    "# simple one with scikit learn (similar to hw 3)\n",
    "\n",
    "# using CountVectorizer\n",
    "LR_CV_model = LogisticRegression(multi_class = 'multinomial')\n",
    "LR_CV_model.fit(trainCVMatr, listTrainStars)\n",
    "\n",
    "# get it to predict\n",
    "LR_CV_prediction = LR_CV_model.predict(testCVMatr)\n",
    "\n",
    "# get accuracy score\n",
    "LR_CV_score = metrics.accuracy_score(listTestStars, LR_CV_prediction)\n",
    "\n",
    "# this is the bit with the tfidf vectorizer\n",
    "LR_TV_model = LogisticRegression(multi_class = 'multinomial')\n",
    "LR_TV_model.fit(trainTVMatr, listTrainStars)\n",
    "\n",
    "# get it to predict\n",
    "LR_TV_prediction = LR_TV_model.predict(testTVMatr)\n",
    "\n",
    "# get accuracy score\n",
    "LR_TV_score = metrics.accuracy_score(listTestStars, LR_TV_prediction)\n",
    "\n",
    "# what do the data say?\n",
    "print(\"Multiclass, logistic regression, CountVectorizer: \" + str(LR_CV_score))\n",
    "print(\"Multiclass, logistic regression, TfidfVectorizer: \" + str(LR_TV_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, CountVectorizer: 0.53\n",
      "Naive Bayes, TfidfVectorizer: 0.53\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB from scikit learn (like hw3)\n",
    "\n",
    "# using CountVectorizer\n",
    "NB_CV_model = MultinomialNB()\n",
    "NB_CV_model.fit(trainCVMatr, listTrainStars)\n",
    "\n",
    "# get it to predict\n",
    "NB_CV_prediction = NB_CV_model.predict(testCVMatr)\n",
    "\n",
    "# get accuracy score\n",
    "NB_CV_score = metrics.accuracy_score(listTestStars, NB_CV_prediction)\n",
    "\n",
    "# this is the bit with the tfidf vectorizer\n",
    "NB_TV_model = MultinomialNB()\n",
    "NB_TV_model.fit(trainCVMatr, listTrainStars)\n",
    "\n",
    "# get it to predict\n",
    "NB_TV_prediction = NB_TV_model.predict(testTVMatr)\n",
    "\n",
    "# get accuracy score\n",
    "NB_TV_score = metrics.accuracy_score(listTestStars, NB_TV_prediction)\n",
    "\n",
    "# what do the data say?\n",
    "print(\"Naive Bayes, CountVectorizer: \" + str(NB_CV_score))\n",
    "print(\"Naive Bayes, TfidfVectorizer: \" + str(NB_TV_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring?\n",
    "- got to take a file\n",
    "- make list of entries\n",
    "- vectorize\n",
    "- LR, NB, vader\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline (VADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit of setup\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion correct: 0.3308\n"
     ]
    }
   ],
   "source": [
    "# getting the compound scores, a mapping from [-1, 1]\n",
    "listOfRes = []\n",
    "\n",
    "data2 = [json.loads(line) for line in open(\"dataset/dataset_en_test.json\", 'r')]\n",
    "\n",
    "for entry in data2:\n",
    "    listOfRes.append(sid.polarity_scores(entry['review_body'])['compound']) \n",
    "\n",
    "\n",
    "#print(listOfRes)\n",
    "\n",
    "\"\"\"\n",
    "brackets for scaling!\n",
    "\n",
    "1: [q0, q1)\n",
    "2: [q1, q2)\n",
    "3: [q3, q4)\n",
    "4: [q4, q5)\n",
    "5: [q5, q6]\n",
    "\"\"\"\n",
    "\n",
    "q0 = -1\n",
    "q1 = -0.6\n",
    "q2 = -0.2\n",
    "q3 = 0.2\n",
    "q4 = 0.6\n",
    "q5 = 1\n",
    "\n",
    "# scaling to [1, 5]\n",
    "numCorrect = 0\n",
    "\n",
    "scaledRes = []\n",
    "for i in range(len(listOfRes)):\n",
    "    num = listOfRes[i]\n",
    "    score = -1\n",
    "    if num >= q0 and num < q1:\n",
    "        score = 1\n",
    "    elif num >= q1 and num < q2:\n",
    "        score = 2\n",
    "    elif num >= q2 and num < q3:\n",
    "        score = 3\n",
    "    elif num >= q3 and num < q4:\n",
    "        score = 4\n",
    "    elif num >= q4 and num <= q5:\n",
    "        score = 5\n",
    "\n",
    "    # add score back in\n",
    "    scaledRes.append(score)\n",
    "    if score == int(data2[i]['stars']):\n",
    "        numCorrect += 1\n",
    "\n",
    "size = len(listOfRes)\n",
    "propCorrect = numCorrect/size\n",
    "\n",
    "print(\"Baseline proportion correct: \" + str(propCorrect))\n",
    "\n",
    "# level of correctness? so if i get a 2 when the answer is 1 is better than a 3+\n",
    "# this is niceeee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product type analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
