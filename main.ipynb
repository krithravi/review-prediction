{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Let's get started!\n",
    "\n",
    "### Design decisions?\n",
    "- Punctuation looks like it's important, so I think we should keep it? like `!!!!` could actually be indicative of something, as might `???`. Periods, maybe not so much, so I think I'm gonna remove those, commas too\n",
    "- It might be better to split up contractions, so I thought that could be n i c e\n",
    "- iffy on stemming and lemmatization. Vader doesn't like it, but we can try it?\n",
    "    - lemmatization: spacy\n",
    "- or, try two versions, one with and the other without, see which one performs better\n",
    "- right, so googles pretrained word2vec exists? to use or not to use, and how?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our imports\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants should we want that\n",
    "\n",
    "# taken from wikipedia + stackexchange answer: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I shall / I will\",\n",
    "    \"I'll've\": \"I shall have / I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of set up\n",
    "lemmatization_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a class with review_id, review_body, stars, product id and category\n",
    "class parsedEntry:\n",
    "    def __init__(self, review_id, sent_text, all_text, stars, product_id, product_category):\n",
    "        self.review_id = review_id\n",
    "        self.sent_text = sent_text\n",
    "        self.all_text = all_text\n",
    "        self.stars = stars\n",
    "        self.product_id = product_id\n",
    "        self.product_category = product_category\n",
    "\n",
    "    def printAll(self):\n",
    "        print(self.review_id)\n",
    "        print(\"\\t TEXT: \" + str(self.all_text))\n",
    "        #print(\"\\t SENT TEXT: \" + str(self.sent_text))\n",
    "        print(\"\\t STARS: \" + str(self.stars))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of parsedEntries\n",
    "def makeListEntries(fileName):\n",
    "    \n",
    "    listEntries = []\n",
    "\n",
    "    for line in open(fileName, 'r'):\n",
    "        entry = json.loads(line)\n",
    "\n",
    "        # GETTING THE PERTINENT BITS #\n",
    "        review_id = entry['review_id']\n",
    "        text = entry['review_body']\n",
    "        stars = entry['stars']\n",
    "        product_id = entry['product_id']\n",
    "        product_category = entry['product_category']\n",
    "\n",
    "        # DEAL WITH THE TEXT PORTION\n",
    "        text = text.lower()\n",
    "        \n",
    "        # breaking up some contractions: globally replace key with value pair\n",
    "        nonCont = text\n",
    "        for key in contractions:\n",
    "            nonCont = re.sub(key, contractions[key], nonCont)\n",
    "\n",
    "        sentences = sent_tokenize(nonCont)\n",
    "\n",
    "        tokenized_sentences = []\n",
    "        tokenized_all = []\n",
    "        for sentence in sentences:\n",
    "            # makes a list type dealio\n",
    "            tokens = lemmatization_model(sentence)\n",
    "\n",
    "           # list just for this sentence\n",
    "            sentenceList = []\n",
    "\n",
    "            for token in tokens:\n",
    "                if (token.lemma_ != \".\" and token.lemma_ != \",\"):\n",
    "                    tokenized_all.append(token.lemma_)\n",
    "                    sentenceList.append(token.lemma_)\n",
    "\n",
    "            # add sentenceList to total sentences\n",
    "            tokenized_sentences.append(sentenceList)\n",
    "\n",
    "        newEntry = parsedEntry(review_id, tokenized_sentences, tokenized_all, stars, product_id, product_category)\n",
    "\n",
    "        listEntries.append(newEntry)\n",
    "    return listEntries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = makeListEntries(\"dataset/smol_train.json\")\n",
    "testSet = makeListEntries(\"dataset/smol_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_0964290\n",
      "\t TEXT: ['arrive', 'broken', 'manufacturer', 'defect', 'two', 'of', 'the', 'leg', 'of', 'the', 'base', 'be', 'not', 'completely', 'form', 'so', 'there', 'be', 'no', 'way', 'to', 'insert', 'the', 'caster', 'I', 'unpackage', 'the', 'entire', 'chair', 'and', 'hardware', 'before', 'notice', 'this', 'so', 'I', \"'ll\", 'spend', 'twice', 'the', 'amount', 'of', 'time', 'box', 'up', 'the', 'whole', 'useless', 'thing', 'and', 'send', 'it', 'back', 'with', 'a', '1', '-', 'star', 'review', 'of', 'part', 'of', 'a', 'chair', 'I', 'never', 'get', 'to', 'sit', 'in', 'I', 'will', 'go', 'so', 'far', 'as', 'to', 'include', 'a', 'picture', 'of', 'what', 'their', 'injection', 'molding', 'and', 'quality', 'assurance', 'process', 'miss', 'though', 'I', 'will', 'be', 'hesitant', 'to', 'buy', 'again', 'it', 'make', 'I', 'wonder', 'if', 'there', 'be', 'not', '/', 'be', 'not', 'miss', 'structure', 'and', 'support', 'that', 'do', 'not', 'impede', 'the', 'assembly', 'process']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0199937\n",
      "\t TEXT: ['these', 'be', 'awful', 'they', 'be', 'see', 'through', 'the', 'fabric', 'feel', 'like', 'tablecloth', 'and', 'they', 'fit', 'like', 'child', '’s', 'clothing', 'customer', 'service', 'do', 'seem', 'to', 'be', 'nice', 'though', 'but', 'I', 'regret', 'miss', 'my', 'return', 'date', 'for', 'these', 'I', 'would', 'n’t', 'even', 'donate', 'they', 'because', 'the', 'quality', 'be', 'so', 'poor']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0690095\n",
      "\t TEXT: ['the', 'cabinet', 'dot', 'be', 'all', 'detach', 'from', 'back', '...', 'get', 'I']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0863335\n",
      "\t TEXT: ['I', 'buy', '4', 'and', 'none', 'of', 'they', 'work', 'yes', 'I', 'use', 'new', 'battery', '!']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0311558\n",
      "\t TEXT: ['I', 'receive', 'my', 'first', 'order', 'of', 'this', 'product', 'and', 'it', 'be', 'break', 'so', 'I', 'order', 'it', 'again', 'the', 'second', 'one', 'be', 'break', 'in', 'more', 'place', 'than', 'the', 'first', 'I', 'can', 'not', 'blame', 'the', 'shipping', 'process', 'as', 'it', 'have', '/', 'it', 'be', 'shrink', 'wrap', 'and', 'box']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0565010\n",
      "\t TEXT: ['on', 'first', 'use', 'it', 'do', 'not', 'heat', 'up', 'and', 'now', 'it', 'do', 'not', 'work', 'at', 'all']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0044972\n",
      "\t TEXT: ['this', 'product', 'be', 'a', 'piece', 'of', 'shit', 'do', 'not', 'buy', 'do', 'not', 'work', 'and', 'then', 'I', 'try', 'to', 'call', 'for', 'customer', 'support', 'it', 'will', 'not', 'take', 'my', 'number', 'fucking', 'rip', 'off', '!']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0963290\n",
      "\t TEXT: ['you', 'want', 'an', 'honest', 'answer', '?', 'I', 'just', 'return', 'from', 'up', 'where', 'I', 'return', 'the', 'farce', 'of', 'an', 'earring', 'set', 'to', 'amazon', 'it', 'do', 'not', 'look', 'like', 'what', 'I', 'see', 'on', 'amazon', 'only', 'a', 'baby', 'would', 'be', 'able', 'to', 'wear', 'the', 'size', 'of', 'the', 'earring', 'they', 'be', 'so', 'small', 'the', 'size', 'of', 'a', 'pin', 'head', 'I', 'at', 'first', 'think', 'amazon', 'have', 'forget', 'to', 'enclose', 'they', 'in', 'the', 'bag', '!', 'I', 'do', 'not', 'bother', 'to', 'take', 'they', 'out', 'of', 'the', 'bag', 'and', 'you', 'can', 'have', 'they', 'back', 'will', 'never', 'order', 'another', 'thing', 'from', 'your', 'company', 'a', 'disgrace', 'honest', 'enough', 'for', 'you', '?', 'grandma']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0784379\n",
      "\t TEXT: ['go', 'through', '3', 'in', 'one', 'day', 'do', 'not', 'fit', 'correct', 'and', 'could', 'not', 'get', 'bubble', 'out', '(', 'well', 'without', ')']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0238156\n",
      "\t TEXT: ['the', 'glue', 'work', 'fine', 'but', 'the', 'container', 'be', 'impossible', 'to', 'work', 'with', 'the', 'cap', 'do', 'not', 'come', 'off', 'without', 'plyer', 'and', 'then', 'will', 'not', 'go', 'back', 'on', 'without', 'a', 'violent', 'abrupt', 'force', 'involve', 'both', 'hand', 'and', 'a', 'solid', 'object', '(', 'desk', 'drawer', ')', 'this', 'happen', 'even', 'though', 'I', 'be', 'careful', 'to', 'not', 'gum', 'up', 'the', 'lid', 'or', 'taper', 'snout']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0420650\n",
      "\t TEXT: ['poor', 'quality', 'the', 'material', 'be', 'fuzzy', 'from', 'day', 'one', 'it', 'got', 'discolor', 'in', 'less', 'than', 'a', 'month', 'even', 'though', 'we', 'keep', 'it', 'outdoors', 'under', 'a', 'cover', 'porch', 'I', 'return', 'this', 'item']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0728007\n",
      "\t TEXT: ['I', 'be', 'over', 'the', 'moon', 'when', 'I', 'get', 'this', 'I', 'love', 'sunflower', 'and', 'it', 'look', 'exactly', 'like', 'they', 'advertise', 'unfortunately', 'I', 'wear', 'this', 'piercing', 'for', 'one', 'weekend', 'by', 'sunday', 'night', 'the', 'sunflower', 'have', 'come', 'off', 'I', 'will', 'not', 'be', 'purchase', 'from', 'this', 'company', 'again', 'very', 'disappointed']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0206383\n",
      "\t TEXT: ['order', '2', 'they', 'ship', '1', 'promise', 'by', 'certain', 'day', 'then', 'the', 'next', 'day', 'then', 'the', 'next', 'day']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0363796\n",
      "\t TEXT: ['get', 'ta', 'say', 'not', 'impressed', 'with', 'the', 'quality', 'I', 'mean', 'I', 'can', 'not', 'say', 'I', 'expect', 'it', 'to', 'be', 'overwhelmingly', 'amazing', 'with', 'the', 'cost', 'be', 'where', 'it', 'have', '/', 'it', 'be', 'at', 'but', 'certainly', 'I', 'think', 'it', 'would', 'last', 'long', 'than', 'a', 'few', 'day', 'have', 'the', 'phone', 'in', 'my', 'gym', 'bag', 'slide', 'out', 'from', 'the', 'side', 'and', 'barely', 'tap', 'the', 'wall', 'on', 'the', 'aluminum', 'side', 'not', 'even', 'on', 'the', 'glass', 'and', 'low', 'and', 'behold', 'instantaneous', 'crack', 'oddly', 'enough', 'just', 'on', 'the', 'back', 'not', 'a', 'splinter', 'at', 'all', 'on', 'the', 'front', 'not', 'sure', 'if', 'the', 'back', 'be', 'intentionally', 'more', 'susceptible', 'to', 'damage', 'or', 'just', 'my', 'luck', 'either', 'way', 'I', 'have', 'to', 'give', 'this', 'product', 'a', 'thumb', 'down', 'wish', 'it', 'hold', 'up', 'well', 'it', 'really', 'be', 'a', 'nice', 'design', 'I', 'be', 'a', 'fan', 'of', 'how', 'the', 'product', 'look', 'and', 'the', 'concept', 'no', 'issue', 'there', 'just', 'not', 'durable', 'at', 'all']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0638563\n",
      "\t TEXT: ['follow', 'direction', 'do', 'not', 'work', 'as', 'advertise']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0254608\n",
      "\t TEXT: ['I', 'would', 'give', 'this', 'zero', 'star', 'if', 'I', 'could', 'this', 'be', '6.3', 'you', 'ounce', 'of', 'low', 'grade', 'chocolate', 'for', '$', '60', 'my', '9', 'year', 'old', 'daughter', 'be', 'so', 'disappointed', 'it', 'do', 'not', 'even', 'have', 'a', 'different', 'piece', 'for', 'each', 'day', 'it', 'be', 'mostly', 'flat', 'square', 'wrap', 'milk', 'chocolate', 'with', 'no', 'design', 'at', 'all', '!', '!', '!', 'very', 'very', 'poor', 'quality', 'and', 'expensive', '!']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0331944\n",
      "\t TEXT: ['there', 'be', 'a', 'terribly', 'do', 'band', 'across', 'the', 'top', 'suppose', 'to', 'keep', 'the', 'shirt', 'on', 'your', 'shoulder', 'but', 'seem', 'more', 'of', 'like', 'a', 'waistband']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0066041\n",
      "\t TEXT: ['two', 'of', 'the', 'glass', 'be', 'break', 'when', 'I', 'open', 'the', 'package', 'could', 'you', 'please', 'be', 'careful', 'for', 'package', 'glass', 'item']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0220290\n",
      "\t TEXT: ['unless', 'you', 'have', 'this', 'jam', '-', 'pack', 'full', 'of', 'item', 'the', 'unit', 'collapse', 'on', 'itself', 'I', 'never', 'hold', 'it', 'have', '/', 'it', 'be', 'shape', 'therefore', 'make', 'it', 'hard', 'to', 'reach', 'my', 'item', 'when', 'I', 'need', 'they', 'the', 'silver', 'metal', 'piece', 'on', 'the', 'handle', 'fall', 'off', 'soon', 'after', 'it', 'arrive', 'I', 'must', 'have', 'miss', 'the', '\"', 'return', 'by', '\"', 'date', 'as', 'I', 'now', 'be', 'stick', 'with', 'it']\n",
      "\t STARS: 1\n",
      "\n",
      "en_0541277\n",
      "\t TEXT: ['do', 'n’t', 'even', 'work', 'do', 'nothing', 'for', 'I', ':(']\n",
      "\t STARS: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    trainSet[i].printAll()\n",
    "    testSet[i].printAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the list of words a string, adds that to a list\n",
    "def makeListText(dataSet):\n",
    "    resList = []\n",
    "    for entry in dataSet:\n",
    "        resList.append(\" \".join(entry.all_text))\n",
    "    return resList\n",
    "\n",
    "# deal with target (the stars) as well\n",
    "def makeListStars(dataSet):\n",
    "    resList = []\n",
    "    for entry in dataSet:\n",
    "        resList.append(entry.stars)\n",
    "    return resList\n",
    "\n",
    "# data\n",
    "listTrainText = makeListText(trainSet)\n",
    "listTestText = makeListText(testSet)\n",
    "\n",
    "# target\n",
    "listTrainStars = makeListStars(trainSet)\n",
    "listTestStars = makeListStars(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could do CountVectorizer\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "trainCVMatr = cv.fit_transform(listTrainText)\n",
    "testCVMatr = cv.transform(listTrainText)\n",
    "\n",
    "#print(trainCVMatr)\n",
    "#print(testCVMatr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TdidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could do TfidfVectorizer\n",
    "tv = TfidfVectorizer()\n",
    "\n",
    "trainTVMatr = cv.fit_transform(listTrainText)\n",
    "testTVMatr = cv.transform(listTrainText)\n",
    "\n",
    "#print(trainTVMatr)\n",
    "#print(testTVMatr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
