{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import mord\n",
    "from sklearn import linear_model, metrics, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants should we want that\n",
    "\n",
    "# taken from wikipedia + stackexchange answer: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"i'd\": \"i had / i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of set up\n",
    "lemmatization_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of entries\n",
    "def makeListEntries(filename):\n",
    "    data = [json.loads(line) for line in open(filename, 'r')]\n",
    "    \n",
    "    for entry in data:\n",
    "        entry['review_body'] = entry['review_body'].lower()\n",
    "        entry['review_body'] = entry['review_body'].replace(\"â€™\", \"'\")\n",
    "\n",
    "        # taking out contractions\n",
    "        for key in contractions:\n",
    "            entry['review_body'] = re.sub(key, contractions[key], entry['review_body'])\n",
    "        \n",
    "        entry['tokenized'] = []\n",
    "        \n",
    "        # removing unnecessary punctuation\n",
    "        tokens = lemmatization_model(entry['review_body'])\n",
    "        entry['tokenized'] = [token.lemma_ for token in tokens if token.lemma_ not in {',', '.'}]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "\n",
    "# makes the list of words a string, adds that to a list\n",
    "def makeListText(dataSet):\n",
    "    resList = []\n",
    "    for entry in dataSet:\n",
    "        resList.append(\" \".join(entry['tokenized']))\n",
    "    return resList\n",
    "\n",
    "# deal with target (the stars) as well\n",
    "def makeListStars(dataSet):\n",
    "    resList = []\n",
    "    for entry in dataSet:\n",
    "        resList.append(int(entry['stars']))\n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vader thresholds for scaling (constants)\n",
    "\"\"\"\n",
    "vader brackets for scaling!\n",
    "\n",
    "1: [q0, q1)\n",
    "2: [q1, q2)\n",
    "3: [q3, q4)\n",
    "4: [q4, q5)\n",
    "5: [q5, q6]\n",
    "\"\"\"\n",
    "q0 = -1\n",
    "q1 = -0.6\n",
    "q2 = -0.2\n",
    "q3 = 0.2\n",
    "q4 = 0.6\n",
    "q5 = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c\n",
    "class OrdinalClassifier():\n",
    "    \n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i,y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                #predicted.append(1 - clfs_predict[y][:,1])\n",
    "                predicted.append(1 - clfs_predict[y-1][:,1])\n",
    "            elif y in clfs_predict:\n",
    "                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n",
    "                #predicted.append(clfs_predict[y-1][:,1] - clfs_predict[y][:,1])\n",
    "                predicted.append(clfs_predict[y-2][:,1] - clfs_predict[y-1][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                #predicted.append(clfs_predict[y-1][:,1])\n",
    "                predicted.append(clfs_predict[y-2][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def betterScoring(true, pred):\n",
    "    ## SCORING GUIDELINES:\n",
    "    # 5 pts if right\n",
    "    # 4 pts if off by 1\n",
    "    # 3 pts if off by 2, etc\n",
    "    size = len(true)\n",
    "    score = 0\n",
    "    \n",
    "    thisdict = {\n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for i in range(size):\n",
    "        offBy = abs(true[i] - pred[i])\n",
    "        thisdict[offBy] += 1\n",
    "        score += 5 - offBy\n",
    "    \n",
    "    for item in thisdict:\n",
    "        thisdict[item] /= size\n",
    "    #return score/(5 * size)\n",
    "    return thisdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doAll(trainFileName, testFileName):\n",
    "    trainSet = makeListEntries(trainFileName)\n",
    "    testSet = makeListEntries(testFileName)\n",
    "    \"\"\"**************************************\"\"\"\n",
    "    # data\n",
    "    listTrainText = makeListText(trainSet)\n",
    "    listTestText = makeListText(testSet)\n",
    "\n",
    "    # target\n",
    "    listTrainStars = makeListStars(trainSet)\n",
    "    listTestStars = makeListStars(testSet)\n",
    "    \"\"\"*************************************\"\"\"\n",
    "    # could do CountVectorizer\n",
    "    cv = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "    trainCVMatr = cv.fit_transform(listTrainText)\n",
    "    testCVMatr = cv.transform(listTestText)\n",
    "\n",
    "    # could do TfidfVectorizer\n",
    "    # tv = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "    # trainTVMatr = cv.fit_transform(listTrainText)\n",
    "    # testTVMatr = cv.transform(listTestText)\n",
    "    \"\"\"*************************************\"\"\"\n",
    "    # using CountVectorizer\n",
    "    LR_CV_model = LogisticRegression(multi_class = 'multinomial', max_iter=1000, class_weight='balanced')\n",
    "    LR_CV_model.fit(trainCVMatr, listTrainStars)\n",
    "\n",
    "    # get it to predict\n",
    "    LR_CV_prediction = LR_CV_model.predict(testCVMatr)\n",
    "\n",
    "    # get accuracy score\n",
    "    LR_CV_score = metrics.accuracy_score(listTestStars, LR_CV_prediction)\n",
    "    LR_CV_f1 = metrics.f1_score(listTestStars, LR_CV_prediction, average='micro')\n",
    "    LR_CV_r2 = metrics.r2_score(listTestStars, LR_CV_prediction, multioutput='variance_weighted')\n",
    "    LR_my = betterScoring(listTestStars, LR_CV_prediction)\n",
    "    # this is the bit with the tfidf vectorizer\n",
    "    # LR_TV_model = LogisticRegression(multi_class = 'multinomial', max_iter=1000)\n",
    "    # LR_TV_model.fit(trainTVMatr, listTrainStars)\n",
    "\n",
    "    # get it to predict\n",
    "    # LR_TV_prediction = LR_TV_model.predict(testTVMatr)\n",
    "\n",
    "    # get accuracy score\n",
    "    # LR_TV_score = metrics.accuracy_score(listTestStars, LR_TV_prediction)\n",
    "\n",
    "    # what do the data say?\n",
    "    #print(\"Multiclass, logistic regression, CountVectorizer: \" + str(LR_CV_score))\n",
    "    #print(\"Multiclass, logistic regression, TfidfVectorizer: \" + str(LR_TV_score))\n",
    "    \"\"\"*************************************\"\"\"\n",
    "    # using CountVectorizer\n",
    "    NB_CV_model = MultinomialNB()\n",
    "    NB_CV_model.fit(trainCVMatr, listTrainStars)\n",
    "\n",
    "    # get it to predict\n",
    "    NB_CV_prediction = NB_CV_model.predict(testCVMatr)\n",
    "\n",
    "    # get accuracy score\n",
    "    NB_CV_score = metrics.accuracy_score(listTestStars, NB_CV_prediction)\n",
    "    NB_CV_f1 = metrics.f1_score(listTestStars, NB_CV_prediction, average='micro')\n",
    "    NB_CV_r2 = metrics.r2_score(listTestStars, NB_CV_prediction, multioutput='variance_weighted')\n",
    "    NB_my = betterScoring(listTestStars, NB_CV_prediction)\n",
    "    # this is the bit with the tfidf vectorizer\n",
    "    # NB_TV_model = MultinomialNB()\n",
    "    # NB_TV_model.fit(trainCVMatr, listTrainStars)\n",
    "\n",
    "    # get it to predict\n",
    "    # NB_TV_prediction = NB_TV_model.predict(testTVMatr)\n",
    "\n",
    "    # get accuracy score\n",
    "    # NB_TV_score = metrics.accuracy_score(listTestStars, NB_TV_prediction)\n",
    "\n",
    "    # what do the data say?\n",
    "    #print(\"Naive Bayes, CountVectorizer: \" + str(NB_CV_score))\n",
    "    # print(\"Naive Bayes, TfidfVectorizer: \" + str(NB_TV_score))\n",
    "    \"\"\"*************************************\"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    listOfRes = []\n",
    "\n",
    "    data2 = [json.loads(line) for line in open(testFileName, 'r')]\n",
    "\n",
    "    for entry in data2:\n",
    "        listOfRes.append(sid.polarity_scores(entry['review_body'])['compound'])\n",
    "\n",
    "    scaledRes = []\n",
    "    size = len(listOfRes)\n",
    "    for i in range(size):\n",
    "        num = listOfRes[i]\n",
    "        score = -1\n",
    "        if num >= q0 and num < q1:\n",
    "            score = 1\n",
    "        elif num >= q1 and num < q2:\n",
    "            score = 2\n",
    "        elif num >= q2 and num < q3:\n",
    "            score = 3\n",
    "        elif num >= q3 and num < q4:\n",
    "            score = 4\n",
    "        elif num >= q4 and num <= q5:\n",
    "            score = 5\n",
    "\n",
    "        # add score back in\n",
    "        scaledRes.append(score)\n",
    "\n",
    "    vader_acc = metrics.accuracy_score(listTestStars, scaledRes)\n",
    "    vader_f1 = metrics.f1_score(listTestStars, scaledRes, average='micro')\n",
    "    vader_r2 = metrics.r2_score(listTestStars, scaledRes, multioutput='variance_weighted')\n",
    "    vader_my = betterScoring(listTestStars, scaledRes)\n",
    "    \"\"\"*************************************\"\"\"\n",
    "    # dealing with the ordinal regression\n",
    "    ord_model = OrdinalClassifier(DecisionTreeClassifier())\n",
    "    ord_model.fit(trainCVMatr, listTrainStars)\n",
    "    ord_model_prediction = ord_model.predict(testCVMatr)\n",
    "    \n",
    "    size = len(listTestStars)\n",
    "    for i in range(size):\n",
    "        if (ord_model_prediction[i] < 1):\n",
    "            ord_model_prediction[i] = 1\n",
    "\n",
    "    ord_acc = metrics.accuracy_score(listTestStars, ord_model_prediction)\n",
    "    ord_f1 = metrics.f1_score(listTestStars, ord_model_prediction, average='micro')\n",
    "    ord_r2 = metrics.r2_score(listTestStars, ord_model_prediction, multioutput='variance_weighted')\n",
    "    ord_my = betterScoring(listTestStars, ord_model_prediction)\n",
    "    \"\"\"*************************************\"\"\"\n",
    "    # trying mord\n",
    "    \n",
    "    arr = np.asarray(listTrainStars)\n",
    "    clf2 = mord.LogisticAT(alpha=1.)\n",
    "    clf2.fit(trainCVMatr, arr)\n",
    "    clf2_prediction = clf2.predict(testCVMatr)\n",
    "    \n",
    "    LAT_acc = metrics.accuracy_score(listTestStars, clf2_prediction)\n",
    "    LAT_f1 = metrics.f1_score(listTestStars, clf2_prediction, average='micro')\n",
    "    LAT_r2 = metrics.r2_score(listTestStars, clf2_prediction, multioutput='variance_weighted')\n",
    "    LAT_my = betterScoring(listTestStars, clf2_prediction)\n",
    "    #print('AccuracyScore of LogisticAT %s' %\n",
    "          #metrics.accuracy_score(listTestStars, clf2.predict(testCVMatr)))\n",
    "    \n",
    "    clf3 = mord.LogisticIT(alpha=1.)\n",
    "    clf3.fit(trainCVMatr, arr)\n",
    "    clf3_prediction = clf3.predict(testCVMatr)\n",
    "    \n",
    "    LIT_acc = metrics.accuracy_score(listTestStars, clf3_prediction)\n",
    "    LIT_f1 = metrics.f1_score(listTestStars, clf3_prediction, average='micro')\n",
    "    LIT_r2 = metrics.r2_score(listTestStars, clf3_prediction, multioutput='variance_weighted')\n",
    "    LIT_my = betterScoring(listTestStars, clf3_prediction)\n",
    "    #print('AccuracyScore of LogisticIT %s' %\n",
    "          #metrics.accuracy_score(listTestStars, clf3.predict(testCVMatr)))\n",
    "\n",
    "    clf4 = mord.LogisticSE(alpha=1.)\n",
    "    clf4.fit(trainCVMatr, arr)\n",
    "    clf4_prediction = clf4.predict(testCVMatr)\n",
    "    \n",
    "    LSE_acc = metrics.accuracy_score(listTestStars, clf4_prediction)\n",
    "    LSE_f1 = metrics.f1_score(listTestStars, clf4_prediction, average='micro')\n",
    "    LSE_r2 = metrics.r2_score(listTestStars, clf4_prediction, multioutput='variance_weighted')\n",
    "    LSE_my = betterScoring(listTestStars, clf4_prediction)\n",
    "    #print('AccuracyScore of LogisticSE %s' %\n",
    "        #metrics.accuracy_score(listTestStars, clf4.predict(testCVMatr)))\n",
    "    \"\"\"*************************************\"\"\"\n",
    "    \n",
    "\n",
    "    # return value\n",
    "    categoryName = trainFileName.replace(\"dataset/prodAnalysis/train_\", \"\")\n",
    "    categoryName = categoryName.replace(\".json\", \"\")\n",
    "    return [categoryName, \n",
    "            LR_CV_score, LR_CV_f1, LR_CV_r2, LR_my, \n",
    "            NB_CV_score, NB_CV_f1, NB_CV_r2, NB_my, \n",
    "            vader_acc, vader_f1, vader_r2, vader_my, \n",
    "            ord_acc, ord_f1, ord_r2, ord_my,\n",
    "            LAT_acc, LAT_f1, LAT_r2, LAT_my, \n",
    "            LIT_acc, LIT_f1, LIT_r2, LIT_my, \n",
    "            LSE_acc, LSE_f1, LSE_r2, LSE_my, \n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/smol_train',\n",
       " 0.47,\n",
       " 0.47,\n",
       " 0.08499999999999996,\n",
       " {0: 0.47, 1: 0.3, 2: 0.15, 3: 0.05, 4: 0.03},\n",
       " 0.53,\n",
       " 0.53,\n",
       " 0.17000000000000004,\n",
       " {0: 0.53, 1: 0.23, 2: 0.16, 3: 0.07, 4: 0.01},\n",
       " 0.38,\n",
       " 0.38,\n",
       " 0.10999999999999999,\n",
       " {0: 0.38, 1: 0.39, 2: 0.15, 3: 0.07, 4: 0.01},\n",
       " 0.27,\n",
       " 0.27,\n",
       " -0.2849999999999999,\n",
       " {0: 0.27, 1: 0.35, 2: 0.24, 3: 0.14, 4: 0.0},\n",
       " 0.48,\n",
       " 0.48,\n",
       " 0.44999999999999996,\n",
       " {0: 0.48, 1: 0.36, 2: 0.14, 3: 0.02, 4: 0.0},\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.39,\n",
       " {0: 0.46, 1: 0.38, 2: 0.12, 3: 0.04, 4: 0.0},\n",
       " 0.46,\n",
       " 0.46,\n",
       " 0.4549999999999999,\n",
       " {0: 0.46, 1: 0.39, 2: 0.13, 3: 0.02, 4: 0.0}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run 'em all\n",
    "doAll(\"dataset/smol_train.json\", \"dataset/smol_test.json\")\n",
    "#doAll(\"dataset/dataset_en_train.json\", \"dataset/dataset_en_test.json\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apparel</th>\n",
       "      <th>automotive</th>\n",
       "      <th>baby_product</th>\n",
       "      <th>beauty</th>\n",
       "      <th>book</th>\n",
       "      <th>camera</th>\n",
       "      <th>digital_ebook_purchase</th>\n",
       "      <th>digital_video_download</th>\n",
       "      <th>drugstore</th>\n",
       "      <th>electronics</th>\n",
       "      <th>...</th>\n",
       "      <th>office_product</th>\n",
       "      <th>other</th>\n",
       "      <th>pc</th>\n",
       "      <th>personal_care_appliances</th>\n",
       "      <th>pet_products</th>\n",
       "      <th>shoes</th>\n",
       "      <th>sports</th>\n",
       "      <th>toy</th>\n",
       "      <th>video_games</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.431677</td>\n",
       "      <td>0.422256</td>\n",
       "      <td>0.392252</td>\n",
       "      <td>0.410490</td>\n",
       "      <td>0.425963</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>0.356354</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403740</td>\n",
       "      <td>0.420874</td>\n",
       "      <td>0.425760</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.392896</td>\n",
       "      <td>0.394273</td>\n",
       "      <td>0.410092</td>\n",
       "      <td>0.460235</td>\n",
       "      <td>0.330144</td>\n",
       "      <td>0.431472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.431677</td>\n",
       "      <td>0.422256</td>\n",
       "      <td>0.392252</td>\n",
       "      <td>0.410490</td>\n",
       "      <td>0.425963</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>0.356354</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403740</td>\n",
       "      <td>0.420874</td>\n",
       "      <td>0.425760</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.392896</td>\n",
       "      <td>0.394273</td>\n",
       "      <td>0.410092</td>\n",
       "      <td>0.460235</td>\n",
       "      <td>0.330144</td>\n",
       "      <td>0.431472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.200479</td>\n",
       "      <td>0.167786</td>\n",
       "      <td>0.105046</td>\n",
       "      <td>0.120785</td>\n",
       "      <td>0.102296</td>\n",
       "      <td>0.083815</td>\n",
       "      <td>-0.091576</td>\n",
       "      <td>-0.059365</td>\n",
       "      <td>0.151751</td>\n",
       "      <td>0.308335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148627</td>\n",
       "      <td>0.136544</td>\n",
       "      <td>0.208756</td>\n",
       "      <td>-1.058372</td>\n",
       "      <td>0.050615</td>\n",
       "      <td>0.109926</td>\n",
       "      <td>0.170483</td>\n",
       "      <td>0.345399</td>\n",
       "      <td>-0.202793</td>\n",
       "      <td>0.191877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{0: 0.43167701863354035, 1: 0.4006211180124223...</td>\n",
       "      <td>{0: 0.4222560975609756, 1: 0.36585365853658536...</td>\n",
       "      <td>{0: 0.3922518159806295, 1: 0.38619854721549635...</td>\n",
       "      <td>{0: 0.4104899497487437, 1: 0.37751256281407036...</td>\n",
       "      <td>{0: 0.4259634888438134, 1: 0.3407707910750507,...</td>\n",
       "      <td>{0: 0.39215686274509803, 1: 0.3636363636363636...</td>\n",
       "      <td>{0: 0.41346153846153844, 1: 0.4343891402714932...</td>\n",
       "      <td>{0: 0.356353591160221, 1: 0.38950276243093923,...</td>\n",
       "      <td>{0: 0.41212710765239946, 1: 0.3654345006485084...</td>\n",
       "      <td>{0: 0.46134969325153374, 1: 0.3570552147239264...</td>\n",
       "      <td>...</td>\n",
       "      <td>{0: 0.4037396121883656, 1: 0.37119113573407203...</td>\n",
       "      <td>{0: 0.420873511060692, 1: 0.3581962563811685, ...</td>\n",
       "      <td>{0: 0.4257602862254025, 1: 0.3643410852713178,...</td>\n",
       "      <td>{0: 0.3, 1: 0.35, 2: 0.1, 3: 0.2, 4: 0.05}</td>\n",
       "      <td>{0: 0.39289558665231433, 1: 0.3638320775026911...</td>\n",
       "      <td>{0: 0.394273127753304, 1: 0.4199706314243759, ...</td>\n",
       "      <td>{0: 0.41009174311926605, 1: 0.3912844036697248...</td>\n",
       "      <td>{0: 0.4602346805736636, 1: 0.3754889178617992,...</td>\n",
       "      <td>{0: 0.33014354066985646, 1: 0.3444976076555024...</td>\n",
       "      <td>{0: 0.43147208121827413, 1: 0.350253807106599,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.434066</td>\n",
       "      <td>0.430386</td>\n",
       "      <td>0.387409</td>\n",
       "      <td>0.417085</td>\n",
       "      <td>0.446247</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.459276</td>\n",
       "      <td>0.383978</td>\n",
       "      <td>0.422827</td>\n",
       "      <td>0.453988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432133</td>\n",
       "      <td>0.420306</td>\n",
       "      <td>0.437686</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.388052</td>\n",
       "      <td>0.409692</td>\n",
       "      <td>0.418807</td>\n",
       "      <td>0.462408</td>\n",
       "      <td>0.354067</td>\n",
       "      <td>0.324873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             apparel  \\\n",
       "0                                           0.431677   \n",
       "1                                           0.431677   \n",
       "2                                           0.200479   \n",
       "3  {0: 0.43167701863354035, 1: 0.4006211180124223...   \n",
       "4                                           0.434066   \n",
       "\n",
       "                                          automotive  \\\n",
       "0                                           0.422256   \n",
       "1                                           0.422256   \n",
       "2                                           0.167786   \n",
       "3  {0: 0.4222560975609756, 1: 0.36585365853658536...   \n",
       "4                                           0.430386   \n",
       "\n",
       "                                        baby_product  \\\n",
       "0                                           0.392252   \n",
       "1                                           0.392252   \n",
       "2                                           0.105046   \n",
       "3  {0: 0.3922518159806295, 1: 0.38619854721549635...   \n",
       "4                                           0.387409   \n",
       "\n",
       "                                              beauty  \\\n",
       "0                                           0.410490   \n",
       "1                                           0.410490   \n",
       "2                                           0.120785   \n",
       "3  {0: 0.4104899497487437, 1: 0.37751256281407036...   \n",
       "4                                           0.417085   \n",
       "\n",
       "                                                book  \\\n",
       "0                                           0.425963   \n",
       "1                                           0.425963   \n",
       "2                                           0.102296   \n",
       "3  {0: 0.4259634888438134, 1: 0.3407707910750507,...   \n",
       "4                                           0.446247   \n",
       "\n",
       "                                              camera  \\\n",
       "0                                           0.392157   \n",
       "1                                           0.392157   \n",
       "2                                           0.083815   \n",
       "3  {0: 0.39215686274509803, 1: 0.3636363636363636...   \n",
       "4                                           0.392157   \n",
       "\n",
       "                              digital_ebook_purchase  \\\n",
       "0                                           0.413462   \n",
       "1                                           0.413462   \n",
       "2                                          -0.091576   \n",
       "3  {0: 0.41346153846153844, 1: 0.4343891402714932...   \n",
       "4                                           0.459276   \n",
       "\n",
       "                              digital_video_download  \\\n",
       "0                                           0.356354   \n",
       "1                                           0.356354   \n",
       "2                                          -0.059365   \n",
       "3  {0: 0.356353591160221, 1: 0.38950276243093923,...   \n",
       "4                                           0.383978   \n",
       "\n",
       "                                           drugstore  \\\n",
       "0                                           0.412127   \n",
       "1                                           0.412127   \n",
       "2                                           0.151751   \n",
       "3  {0: 0.41212710765239946, 1: 0.3654345006485084...   \n",
       "4                                           0.422827   \n",
       "\n",
       "                                         electronics  ...  \\\n",
       "0                                           0.461350  ...   \n",
       "1                                           0.461350  ...   \n",
       "2                                           0.308335  ...   \n",
       "3  {0: 0.46134969325153374, 1: 0.3570552147239264...  ...   \n",
       "4                                           0.453988  ...   \n",
       "\n",
       "                                      office_product  \\\n",
       "0                                           0.403740   \n",
       "1                                           0.403740   \n",
       "2                                           0.148627   \n",
       "3  {0: 0.4037396121883656, 1: 0.37119113573407203...   \n",
       "4                                           0.432133   \n",
       "\n",
       "                                               other  \\\n",
       "0                                           0.420874   \n",
       "1                                           0.420874   \n",
       "2                                           0.136544   \n",
       "3  {0: 0.420873511060692, 1: 0.3581962563811685, ...   \n",
       "4                                           0.420306   \n",
       "\n",
       "                                                  pc  \\\n",
       "0                                           0.425760   \n",
       "1                                           0.425760   \n",
       "2                                           0.208756   \n",
       "3  {0: 0.4257602862254025, 1: 0.3643410852713178,...   \n",
       "4                                           0.437686   \n",
       "\n",
       "                     personal_care_appliances  \\\n",
       "0                                    0.300000   \n",
       "1                                    0.300000   \n",
       "2                                   -1.058372   \n",
       "3  {0: 0.3, 1: 0.35, 2: 0.1, 3: 0.2, 4: 0.05}   \n",
       "4                                    0.250000   \n",
       "\n",
       "                                        pet_products  \\\n",
       "0                                           0.392896   \n",
       "1                                           0.392896   \n",
       "2                                           0.050615   \n",
       "3  {0: 0.39289558665231433, 1: 0.3638320775026911...   \n",
       "4                                           0.388052   \n",
       "\n",
       "                                               shoes  \\\n",
       "0                                           0.394273   \n",
       "1                                           0.394273   \n",
       "2                                           0.109926   \n",
       "3  {0: 0.394273127753304, 1: 0.4199706314243759, ...   \n",
       "4                                           0.409692   \n",
       "\n",
       "                                              sports  \\\n",
       "0                                           0.410092   \n",
       "1                                           0.410092   \n",
       "2                                           0.170483   \n",
       "3  {0: 0.41009174311926605, 1: 0.3912844036697248...   \n",
       "4                                           0.418807   \n",
       "\n",
       "                                                 toy  \\\n",
       "0                                           0.460235   \n",
       "1                                           0.460235   \n",
       "2                                           0.345399   \n",
       "3  {0: 0.4602346805736636, 1: 0.3754889178617992,...   \n",
       "4                                           0.462408   \n",
       "\n",
       "                                         video_games  \\\n",
       "0                                           0.330144   \n",
       "1                                           0.330144   \n",
       "2                                          -0.202793   \n",
       "3  {0: 0.33014354066985646, 1: 0.3444976076555024...   \n",
       "4                                           0.354067   \n",
       "\n",
       "                                               watch  \n",
       "0                                           0.431472  \n",
       "1                                           0.431472  \n",
       "2                                           0.191877  \n",
       "3  {0: 0.43147208121827413, 1: 0.350253807106599,...  \n",
       "4                                           0.324873  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equalizing - 75-25 split\n",
    "\n",
    "listSubFiles = [\n",
    "    [\"dataset/prodAnalysis/train_entire.json\", \"dataset/prodAnalysis/test_entire.json\"],\n",
    "    [\"dataset/prodAnalysis/train_apparel.json\", \"dataset/prodAnalysis/test_apparel.json\"],\n",
    "    [\"dataset/prodAnalysis/train_automotive.json\", \"dataset/prodAnalysis/test_automotive.json\"],\n",
    "    [\"dataset/prodAnalysis/train_baby_product.json\", \"dataset/prodAnalysis/test_baby_product.json\"],\n",
    "    [\"dataset/prodAnalysis/train_beauty.json\", \"dataset/prodAnalysis/test_beauty.json\"],\n",
    "    [\"dataset/prodAnalysis/train_book.json\", \"dataset/prodAnalysis/test_book.json\"],\n",
    "    [\"dataset/prodAnalysis/train_camera.json\", \"dataset/prodAnalysis/test_camera.json\"],\n",
    "    [\"dataset/prodAnalysis/train_digital_ebook_purchase.json\", \"dataset/prodAnalysis/test_digital_ebook_purchase.json\"],\n",
    "    [\"dataset/prodAnalysis/train_digital_video_download.json\", \"dataset/prodAnalysis/test_digital_video_download.json\"],\n",
    "    [\"dataset/prodAnalysis/train_drugstore.json\", \"dataset/prodAnalysis/test_drugstore.json\"],\n",
    "    [\"dataset/prodAnalysis/train_electronics.json\", \"dataset/prodAnalysis/test_electronics.json\"],\n",
    "    [\"dataset/prodAnalysis/train_furniture.json\", \"dataset/prodAnalysis/test_furniture.json\"],\n",
    "    [\"dataset/prodAnalysis/train_grocery.json\", \"dataset/prodAnalysis/test_grocery.json\"],\n",
    "    [\"dataset/prodAnalysis/train_home.json\", \"dataset/prodAnalysis/test_home.json\"],\n",
    "    [\"dataset/prodAnalysis/train_home_improvement.json\", \"dataset/prodAnalysis/test_home_improvement.json\"],\n",
    "    [\"dataset/prodAnalysis/train_industrial_supplies.json\", \"dataset/prodAnalysis/test_industrial_supplies.json\"],\n",
    "    [\"dataset/prodAnalysis/train_jewelry.json\", \"dataset/prodAnalysis/test_jewelry.json\"],\n",
    "    [\"dataset/prodAnalysis/train_kitchen.json\", \"dataset/prodAnalysis/test_kitchen.json\"],\n",
    "    [\"dataset/prodAnalysis/train_lawn_and_garden.json\", \"dataset/prodAnalysis/test_lawn_and_garden.json\"],\n",
    "    [\"dataset/prodAnalysis/train_luggage.json\", \"dataset/prodAnalysis/test_luggage.json\"],\n",
    "    [\"dataset/prodAnalysis/train_musical_instruments.json\", \"dataset/prodAnalysis/test_musical_instruments.json\"],\n",
    "    [\"dataset/prodAnalysis/train_office_product.json\", \"dataset/prodAnalysis/test_office_product.json\"],\n",
    "    [\"dataset/prodAnalysis/train_other.json\", \"dataset/prodAnalysis/test_other.json\"],\n",
    "    [\"dataset/prodAnalysis/train_pc.json\", \"dataset/prodAnalysis/test_pc.json\"],\n",
    "    [\"dataset/prodAnalysis/train_personal_care_appliances.json\", \"dataset/prodAnalysis/test_personal_care_appliances.json\"],\n",
    "    [\"dataset/prodAnalysis/train_pet_products.json\", \"dataset/prodAnalysis/test_pet_products.json\"],\n",
    "    [\"dataset/prodAnalysis/train_shoes.json\", \"dataset/prodAnalysis/test_shoes.json\"],\n",
    "    [\"dataset/prodAnalysis/train_sports.json\", \"dataset/prodAnalysis/test_sports.json\"],\n",
    "    [\"dataset/prodAnalysis/train_toy.json\", \"dataset/prodAnalysis/test_toy.json\"],\n",
    "    [\"dataset/prodAnalysis/train_video_games.json\", \"dataset/prodAnalysis/test_video_games.json\"],\n",
    "    [\"dataset/prodAnalysis/train_watch.json\", \"dataset/prodAnalysis/test_watch.json\"],\n",
    "    [\"dataset/prodAnalysis/train_wireless.json\", \"dataset/prodAnalysis/test_wireless.json\"]\n",
    "]\n",
    "\n",
    "\n",
    "largeDf = pd.DataFrame()\n",
    "for i in range(1,31):\n",
    "    list = doAll(listSubFiles[i][0], listSubFiles[i][1])\n",
    "    largeDf[list[0]] = list[1:]\n",
    "\n",
    "largeDf.head()\n",
    "#largeDf.to_csv(path_or_buf=\"yeehaw_stopwords.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlargeDf = largeDf.T\n",
    "i = 0\n",
    "index = {}\n",
    "\n",
    "rowList = []\n",
    "\n",
    "for category in largeDf.columns:\n",
    "    #print(category)\n",
    "    dictDfLR = tlargeDf.loc[category][3]\n",
    "    dictDfNB = tlargeDf.loc[category][7]\n",
    "    dictDfVad = tlargeDf.loc[category][11]\n",
    "\n",
    "    offByDf = {}\n",
    "    offByDf[\"LR0\"] = dictDfLR[0]\n",
    "    offByDf[\"LR1\"] = dictDfLR[1]\n",
    "    offByDf[\"LR2\"] = dictDfLR[2]\n",
    "    offByDf[\"LR3\"] = dictDfLR[3]\n",
    "    offByDf[\"LR4\"] = dictDfLR[4]\n",
    "\n",
    "    offByDf[\"NB0\"] = dictDfNB[0]\n",
    "    offByDf[\"NB1\"] = dictDfNB[1]\n",
    "    offByDf[\"NB2\"] = dictDfNB[2]\n",
    "    offByDf[\"NB3\"] = dictDfNB[3]\n",
    "    offByDf[\"NB4\"] = dictDfNB[4]\n",
    "\n",
    "    offByDf[\"Vad0\"] = dictDfVad[0]\n",
    "    offByDf[\"Vad1\"] = dictDfVad[1]\n",
    "    offByDf[\"Vad2\"] = dictDfVad[2]\n",
    "    offByDf[\"Vad3\"] = dictDfVad[3]\n",
    "    offByDf[\"Vad4\"] = dictDfVad[4]\n",
    "\n",
    "    rowList.append(offByDf)\n",
    "\n",
    "    index[i] = category\n",
    "    i += 1\n",
    "\n",
    "\n",
    "fullOffByDf = pd.DataFrame(rowList)\n",
    "fullOffByDf.rename(index=index, inplace=True)\n",
    "fullOffByDf.head()\n",
    "fullOffByDf.to_csv(path_or_buf=\"offBy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
